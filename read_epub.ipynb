{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chap_as_list(items):\n",
    "    #filter out and store all chapters in a list\n",
    "    chapters = []\n",
    "    for c in items:\n",
    "        if 'chap' in c.get_name():\n",
    "            chapters.append(c)\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_func(words):\n",
    "    return len(nlp(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_word_count(text, word_count):\n",
    "    #checks for the length of text passed in, splitting if text is longer than word_count\n",
    "    segments = []\n",
    "    curr_segment = []\n",
    "    curr_word_count = 0\n",
    "\n",
    "    for token in text.split():\n",
    "        curr_word_count += 1\n",
    "        curr_segment.append(token.text)\n",
    "\n",
    "        if curr_word_count >= word_count:\n",
    "            segments.append(\" \".join(curr_segment))\n",
    "            curr_segment = []\n",
    "            curr_word_count = 0\n",
    "\n",
    "    # Append remaining chunk to the last segment if the chunk is longer than 50 words\n",
    "    if curr_word_count > 50:\n",
    "        segments.append(\" \".join(curr_segment))\n",
    "\n",
    "    #returns a list of segments, all under set word count\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use BeautifulSoup to read out only the text in each chapter, saved as a list of paragraphs\n",
    "def chapter_to_str(chapter):\n",
    "    soup = BeautifulSoup(chapter.get_body_content(), 'html.parser')\n",
    "    text = [para.get_text() for para in soup.find_all('p')]\n",
    "    for paragraph in text:\n",
    "        text += split_text_by_word_count(paragraph, 250)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(chapters):\n",
    "    #save chunks into dictionary\n",
    "    texts = {}\n",
    "    for c in chapters:\n",
    "        texts[c.get_name()] = chapter_to_str(c)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def main():\\n    #opens source file and returns pickled dictionary of text in chunks of length 50-200\\n    book = epub.read_epub(sys.argv[1])\\n    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\\n    #save all chapters in a list\\n    chapters = chap_as_list(items)\\n    #chunk texts, limiting length\\n    chunks = chunk(chapters)\\n    #print(chunks['OEBPS/Oblivion_chap_1.html'][61])\\n    return chunks\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def main():\n",
    "    #opens source file and returns pickled dictionary of text in chunks of length 50-200\n",
    "    book = epub.read_epub(sys.argv[1])\n",
    "    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\n",
    "    #save all chapters in a list\n",
    "    chapters = chap_as_list(items)\n",
    "    #chunk texts, limiting length\n",
    "    chunks = chunk(chapters)\n",
    "    return chunks\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to help iterate through files in a folder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../wdf_epubs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master dictionary with all books chunked in their respective sublists\n",
    "book_chunks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../wdf_epubs/oblivion.epub\n",
      "['oblivion.epub', 'Consider the Lobster & Other Essays (Little Brown, 2006).epub', 'Brief Interviews with Hideous Men (Little Brown, 1999).epub', 'Infinite Jest (Little Brown, 1996).epub', 'Last Interview & Other Conversations (Melville House, 2012).epub', 'Pale King, The (Little Brown, 2011).epub', \"A Supposedly Fun Thing I'll Never Do Again (Little Brown, 1997).epub\", 'Fate, Time, and Language (Columbia, 2011).epub', 'Girl with Curious Hair (Norton, 1989).epub', 'Both Flesh and Not (Little Brown, 2012).epub', 'Up, Simba! (Little Brown, 2000).epub', 'Broom of the System (Penguin, 1987).epub']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ql/480bqgd56wz4x3b25hcy0bhr0000gn/T/ipykernel_15069/3153632213.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mchapters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchap_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#chunk texts, limiting length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mbook_chunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ql/480bqgd56wz4x3b25hcy0bhr0000gn/T/ipykernel_15069/3385458882.py\u001b[0m in \u001b[0;36mchunk\u001b[0;34m(chapters)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchapters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchapter_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ql/480bqgd56wz4x3b25hcy0bhr0000gn/T/ipykernel_15069/1998137477.py\u001b[0m in \u001b[0;36mchapter_to_str\u001b[0;34m(chapter)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msplit_text_by_word_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ql/480bqgd56wz4x3b25hcy0bhr0000gn/T/ipykernel_15069/427047598.py\u001b[0m in \u001b[0;36msplit_text_by_word_count\u001b[0;34m(text, word_count)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcurr_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcurr_segment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurr_word_count\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for book_name in os.listdir(folder_path):\n",
    "    book = epub.read_epub('../wdf_epubs/' + book_name)\n",
    "    items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\n",
    "    #save all chapters in a list\n",
    "    chapters = chap_as_list(items)\n",
    "    #chunk texts, limiting length\n",
    "    chunks = chunk(chapters)\n",
    "    book_chunks['book_name'] = chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
